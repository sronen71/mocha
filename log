I0127 23:59:14.983003 29686 caffe.cpp:99] Use GPU with device ID 0
I0127 23:59:15.107087 29686 caffe.cpp:107] Starting Optimization
I0127 23:59:15.107161 29686 solver.cpp:32] Initializing solver from parameters: 
test_iter: 607
test_interval: 1000
base_lr: 0.01
display: 500
max_iter: 50000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 16000
snapshot: 5000
snapshot_prefix: "examples/plankton/inet"
solver_mode: GPU
net: "examples/plankton/inet_train_test.prototxt"
I0127 23:59:15.107182 29686 solver.cpp:67] Creating training net from net file: examples/plankton/inet_train_test.prototxt
I0127 23:59:15.107588 29686 net.cpp:278] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0127 23:59:15.107708 29686 net.cpp:39] Initializing net from parameters: 
name: "CaffeNet"
layers {
  top: "data"
  top: "label"
  name: "data"
  type: DATA
  data_param {
    source: "examples/plankton/plankton_train_lmdb"
    batch_size: 256
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 56
    rotate: true
    resize: 64
    randsize: true
    distort: true
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv1b"
  name: "conv1b"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.02
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layers {
  bottom: "conv1b"
  top: "conv1b"
  name: "relu1b"
  type: RELU
}
layers {
  bottom: "conv1b"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.02
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "conv2b"
  name: "conv2b"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.02
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layers {
  bottom: "conv2b"
  top: "conv2b"
  name: "relu2b"
  type: RELU
}
layers {
  bottom: "conv2b"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.02
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layers {
  bottom: "conv3"
  top: "conv3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "conv4"
  name: "conv4"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.02
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layers {
  bottom: "conv4"
  top: "conv4"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "conv4"
  top: "pool4"
  name: "pool4"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool4"
  top: "pool4x"
  name: "predrop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool4x"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 1024
    weight_filler {
      type: "gaussian"
      std: 0.02
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layers {
  bottom: "fc1"
  top: "fc1"
  name: "fcrelu1"
  type: RELU
}
layers {
  bottom: "fc1"
  top: "fc1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc1"
  top: "fc2"
  name: "fc2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.02
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layers {
  bottom: "fc2"
  top: "fc2"
  name: "fcrelu2"
  type: RELU
}
layers {
  bottom: "fc2"
  top: "fc2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc2"
  top: "fc3"
  name: "fc3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 121
    weight_filler {
      type: "gaussian"
      std: 0.02
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layers {
  bottom: "fc3"
  top: "pseudoLabel"
  name: "argmax"
  type: ARGMAX
}
layers {
  bottom: "fc3"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
layers {
  bottom: "fc3"
  bottom: "pseudoLabel"
  top: "pseudoLoss"
  name: "psuedoLoss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I0127 23:59:15.107831 29686 net.cpp:67] Creating Layer data
I0127 23:59:15.107841 29686 net.cpp:359] data -> data
I0127 23:59:15.107852 29686 net.cpp:359] data -> label
I0127 23:59:15.107861 29686 net.cpp:96] Setting up data
I0127 23:59:15.107923 29686 data_layer.cpp:68] Opening lmdb examples/plankton/plankton_train_lmdb
I0127 23:59:15.107969 29686 data_layer.cpp:128] output data size: 256,1,56,56
I0127 23:59:15.108989 29686 net.cpp:103] Top shape: 256 1 56 56 (802816)
I0127 23:59:15.108996 29686 net.cpp:103] Top shape: 256 1 1 1 (256)
I0127 23:59:15.109004 29686 net.cpp:67] Creating Layer conv1
I0127 23:59:15.109007 29686 net.cpp:397] conv1 <- data
I0127 23:59:15.109025 29686 net.cpp:359] conv1 -> conv1
I0127 23:59:15.109032 29686 net.cpp:96] Setting up conv1
I0127 23:59:15.122192 29686 net.cpp:103] Top shape: 256 64 56 56 (51380224)
I0127 23:59:15.122228 29686 net.cpp:67] Creating Layer relu1
I0127 23:59:15.122234 29686 net.cpp:397] relu1 <- conv1
I0127 23:59:15.122239 29686 net.cpp:348] relu1 -> conv1 (in-place)
I0127 23:59:15.122246 29686 net.cpp:96] Setting up relu1
I0127 23:59:15.122254 29686 net.cpp:103] Top shape: 256 64 56 56 (51380224)
I0127 23:59:15.122259 29686 net.cpp:67] Creating Layer norm1
I0127 23:59:15.122263 29686 net.cpp:397] norm1 <- conv1
I0127 23:59:15.122268 29686 net.cpp:359] norm1 -> norm1
I0127 23:59:15.122274 29686 net.cpp:96] Setting up norm1
I0127 23:59:15.122279 29686 net.cpp:103] Top shape: 256 64 56 56 (51380224)
I0127 23:59:15.122285 29686 net.cpp:67] Creating Layer conv1b
I0127 23:59:15.122289 29686 net.cpp:397] conv1b <- norm1
I0127 23:59:15.122293 29686 net.cpp:359] conv1b -> conv1b
I0127 23:59:15.122299 29686 net.cpp:96] Setting up conv1b
I0127 23:59:15.123297 29686 net.cpp:103] Top shape: 256 64 56 56 (51380224)
I0127 23:59:15.123311 29686 net.cpp:67] Creating Layer relu1b
I0127 23:59:15.123316 29686 net.cpp:397] relu1b <- conv1b
I0127 23:59:15.123319 29686 net.cpp:348] relu1b -> conv1b (in-place)
I0127 23:59:15.123332 29686 net.cpp:96] Setting up relu1b
I0127 23:59:15.123342 29686 net.cpp:103] Top shape: 256 64 56 56 (51380224)
I0127 23:59:15.123347 29686 net.cpp:67] Creating Layer pool1
I0127 23:59:15.123352 29686 net.cpp:397] pool1 <- conv1b
I0127 23:59:15.123355 29686 net.cpp:359] pool1 -> pool1
I0127 23:59:15.123359 29686 net.cpp:96] Setting up pool1
I0127 23:59:15.123374 29686 net.cpp:103] Top shape: 256 64 28 28 (12845056)
I0127 23:59:15.123380 29686 net.cpp:67] Creating Layer conv2
I0127 23:59:15.123383 29686 net.cpp:397] conv2 <- pool1
I0127 23:59:15.123388 29686 net.cpp:359] conv2 -> conv2
I0127 23:59:15.123392 29686 net.cpp:96] Setting up conv2
I0127 23:59:15.128046 29686 net.cpp:103] Top shape: 256 128 28 28 (25690112)
I0127 23:59:15.128059 29686 net.cpp:67] Creating Layer relu2
I0127 23:59:15.128063 29686 net.cpp:397] relu2 <- conv2
I0127 23:59:15.128067 29686 net.cpp:348] relu2 -> conv2 (in-place)
I0127 23:59:15.128072 29686 net.cpp:96] Setting up relu2
I0127 23:59:15.128077 29686 net.cpp:103] Top shape: 256 128 28 28 (25690112)
I0127 23:59:15.128082 29686 net.cpp:67] Creating Layer conv2b
I0127 23:59:15.128087 29686 net.cpp:397] conv2b <- conv2
I0127 23:59:15.128089 29686 net.cpp:359] conv2b -> conv2b
I0127 23:59:15.128094 29686 net.cpp:96] Setting up conv2b
I0127 23:59:15.131428 29686 net.cpp:103] Top shape: 256 128 28 28 (25690112)
I0127 23:59:15.131438 29686 net.cpp:67] Creating Layer relu2b
I0127 23:59:15.131441 29686 net.cpp:397] relu2b <- conv2b
I0127 23:59:15.131444 29686 net.cpp:348] relu2b -> conv2b (in-place)
I0127 23:59:15.131449 29686 net.cpp:96] Setting up relu2b
I0127 23:59:15.131453 29686 net.cpp:103] Top shape: 256 128 28 28 (25690112)
I0127 23:59:15.131458 29686 net.cpp:67] Creating Layer pool2
I0127 23:59:15.131461 29686 net.cpp:397] pool2 <- conv2b
I0127 23:59:15.131465 29686 net.cpp:359] pool2 -> pool2
I0127 23:59:15.131470 29686 net.cpp:96] Setting up pool2
I0127 23:59:15.131475 29686 net.cpp:103] Top shape: 256 128 14 14 (6422528)
I0127 23:59:15.131481 29686 net.cpp:67] Creating Layer conv3
I0127 23:59:15.131484 29686 net.cpp:397] conv3 <- pool2
I0127 23:59:15.131489 29686 net.cpp:359] conv3 -> conv3
I0127 23:59:15.131492 29686 net.cpp:96] Setting up conv3
I0127 23:59:15.138144 29686 net.cpp:103] Top shape: 256 256 14 14 (12845056)
I0127 23:59:15.138156 29686 net.cpp:67] Creating Layer relu3
I0127 23:59:15.138160 29686 net.cpp:397] relu3 <- conv3
I0127 23:59:15.138164 29686 net.cpp:348] relu3 -> conv3 (in-place)
I0127 23:59:15.138169 29686 net.cpp:96] Setting up relu3
I0127 23:59:15.138173 29686 net.cpp:103] Top shape: 256 256 14 14 (12845056)
I0127 23:59:15.138178 29686 net.cpp:67] Creating Layer pool3
I0127 23:59:15.138181 29686 net.cpp:397] pool3 <- conv3
I0127 23:59:15.138185 29686 net.cpp:359] pool3 -> pool3
I0127 23:59:15.138190 29686 net.cpp:96] Setting up pool3
I0127 23:59:15.138193 29686 net.cpp:103] Top shape: 256 256 7 7 (3211264)
I0127 23:59:15.138200 29686 net.cpp:67] Creating Layer conv4
I0127 23:59:15.138202 29686 net.cpp:397] conv4 <- pool3
I0127 23:59:15.138206 29686 net.cpp:359] conv4 -> conv4
I0127 23:59:15.138211 29686 net.cpp:96] Setting up conv4
I0127 23:59:15.164474 29686 net.cpp:103] Top shape: 256 512 7 7 (6422528)
I0127 23:59:15.164496 29686 net.cpp:67] Creating Layer relu4
I0127 23:59:15.164501 29686 net.cpp:397] relu4 <- conv4
I0127 23:59:15.164506 29686 net.cpp:348] relu4 -> conv4 (in-place)
I0127 23:59:15.164511 29686 net.cpp:96] Setting up relu4
I0127 23:59:15.164516 29686 net.cpp:103] Top shape: 256 512 7 7 (6422528)
I0127 23:59:15.164522 29686 net.cpp:67] Creating Layer pool4
I0127 23:59:15.164525 29686 net.cpp:397] pool4 <- conv4
I0127 23:59:15.164530 29686 net.cpp:359] pool4 -> pool4
I0127 23:59:15.164535 29686 net.cpp:96] Setting up pool4
I0127 23:59:15.164541 29686 net.cpp:103] Top shape: 256 512 3 3 (1179648)
I0127 23:59:15.164546 29686 net.cpp:67] Creating Layer predrop
I0127 23:59:15.164548 29686 net.cpp:397] predrop <- pool4
I0127 23:59:15.164552 29686 net.cpp:359] predrop -> pool4x
I0127 23:59:15.164562 29686 net.cpp:96] Setting up predrop
I0127 23:59:15.164574 29686 net.cpp:103] Top shape: 256 512 3 3 (1179648)
I0127 23:59:15.164579 29686 net.cpp:67] Creating Layer fc1
I0127 23:59:15.164582 29686 net.cpp:397] fc1 <- pool4x
I0127 23:59:15.164588 29686 net.cpp:359] fc1 -> fc1
I0127 23:59:15.164592 29686 net.cpp:96] Setting up fc1
I0127 23:59:15.268318 29686 net.cpp:103] Top shape: 256 1024 1 1 (262144)
I0127 23:59:15.268345 29686 net.cpp:67] Creating Layer fcrelu1
I0127 23:59:15.268349 29686 net.cpp:397] fcrelu1 <- fc1
I0127 23:59:15.268354 29686 net.cpp:348] fcrelu1 -> fc1 (in-place)
I0127 23:59:15.268359 29686 net.cpp:96] Setting up fcrelu1
I0127 23:59:15.268369 29686 net.cpp:103] Top shape: 256 1024 1 1 (262144)
I0127 23:59:15.268375 29686 net.cpp:67] Creating Layer drop1
I0127 23:59:15.268378 29686 net.cpp:397] drop1 <- fc1
I0127 23:59:15.268383 29686 net.cpp:348] drop1 -> fc1 (in-place)
I0127 23:59:15.268386 29686 net.cpp:96] Setting up drop1
I0127 23:59:15.268389 29686 net.cpp:103] Top shape: 256 1024 1 1 (262144)
I0127 23:59:15.268394 29686 net.cpp:67] Creating Layer fc2
I0127 23:59:15.268398 29686 net.cpp:397] fc2 <- fc1
I0127 23:59:15.268401 29686 net.cpp:359] fc2 -> fc2
I0127 23:59:15.268406 29686 net.cpp:96] Setting up fc2
I0127 23:59:15.280105 29686 net.cpp:103] Top shape: 256 512 1 1 (131072)
I0127 23:59:15.280119 29686 net.cpp:67] Creating Layer fcrelu2
I0127 23:59:15.280122 29686 net.cpp:397] fcrelu2 <- fc2
I0127 23:59:15.280127 29686 net.cpp:348] fcrelu2 -> fc2 (in-place)
I0127 23:59:15.280130 29686 net.cpp:96] Setting up fcrelu2
I0127 23:59:15.280138 29686 net.cpp:103] Top shape: 256 512 1 1 (131072)
I0127 23:59:15.280143 29686 net.cpp:67] Creating Layer drop2
I0127 23:59:15.280145 29686 net.cpp:397] drop2 <- fc2
I0127 23:59:15.280149 29686 net.cpp:348] drop2 -> fc2 (in-place)
I0127 23:59:15.280153 29686 net.cpp:96] Setting up drop2
I0127 23:59:15.280156 29686 net.cpp:103] Top shape: 256 512 1 1 (131072)
I0127 23:59:15.280160 29686 net.cpp:67] Creating Layer fc3
I0127 23:59:15.280164 29686 net.cpp:397] fc3 <- fc2
I0127 23:59:15.280169 29686 net.cpp:359] fc3 -> fc3
I0127 23:59:15.280174 29686 net.cpp:96] Setting up fc3
I0127 23:59:15.281561 29686 net.cpp:103] Top shape: 256 121 1 1 (30976)
I0127 23:59:15.281570 29686 net.cpp:67] Creating Layer fc3_fc3_0_split
I0127 23:59:15.281574 29686 net.cpp:397] fc3_fc3_0_split <- fc3
I0127 23:59:15.281579 29686 net.cpp:359] fc3_fc3_0_split -> fc3_fc3_0_split_0
I0127 23:59:15.281582 29686 net.cpp:359] fc3_fc3_0_split -> fc3_fc3_0_split_1
I0127 23:59:15.281587 29686 net.cpp:359] fc3_fc3_0_split -> fc3_fc3_0_split_2
I0127 23:59:15.281592 29686 net.cpp:96] Setting up fc3_fc3_0_split
I0127 23:59:15.281599 29686 net.cpp:103] Top shape: 256 121 1 1 (30976)
I0127 23:59:15.281601 29686 net.cpp:103] Top shape: 256 121 1 1 (30976)
I0127 23:59:15.281604 29686 net.cpp:103] Top shape: 256 121 1 1 (30976)
I0127 23:59:15.281607 29686 net.cpp:67] Creating Layer argmax
I0127 23:59:15.281610 29686 net.cpp:397] argmax <- fc3_fc3_0_split_0
I0127 23:59:15.281615 29686 net.cpp:359] argmax -> pseudoLabel
I0127 23:59:15.281620 29686 net.cpp:96] Setting up argmax
I0127 23:59:15.281623 29686 net.cpp:103] Top shape: 256 1 1 1 (256)
I0127 23:59:15.281628 29686 net.cpp:67] Creating Layer loss
I0127 23:59:15.281631 29686 net.cpp:397] loss <- fc3_fc3_0_split_1
I0127 23:59:15.281635 29686 net.cpp:397] loss <- label
I0127 23:59:15.281640 29686 net.cpp:359] loss -> loss
I0127 23:59:15.281643 29686 net.cpp:96] Setting up loss
I0127 23:59:15.281652 29686 net.cpp:103] Top shape: 1 1 1 1 (1)
I0127 23:59:15.281656 29686 net.cpp:109]     with loss weight 1
I0127 23:59:15.281682 29686 net.cpp:67] Creating Layer psuedoLoss
I0127 23:59:15.281685 29686 net.cpp:397] psuedoLoss <- fc3_fc3_0_split_2
I0127 23:59:15.281689 29686 net.cpp:397] psuedoLoss <- pseudoLabel
I0127 23:59:15.281693 29686 net.cpp:359] psuedoLoss -> pseudoLoss
I0127 23:59:15.281697 29686 net.cpp:96] Setting up psuedoLoss
I0127 23:59:15.281702 29686 net.cpp:103] Top shape: 1 1 1 1 (1)
I0127 23:59:15.281705 29686 net.cpp:109]     with loss weight 1
I0127 23:59:15.281715 29686 net.cpp:172] psuedoLoss needs backward computation.
blobs under loss: fc3_fc3_0_split_2
blobs under loss: pseudoLabel
I0127 23:59:15.281740 29686 net.cpp:172] loss needs backward computation.
blobs under loss: fc3_fc3_0_split_1
blobs under loss: label
I0127 23:59:15.281745 29686 net.cpp:172] argmax needs backward computation.
blobs under loss: fc3_fc3_0_split_0
I0127 23:59:15.281749 29686 net.cpp:172] fc3_fc3_0_split needs backward computation.
blobs under loss: fc3
I0127 23:59:15.281754 29686 net.cpp:172] fc3 needs backward computation.
blobs under loss: fc2
I0127 23:59:15.281757 29686 net.cpp:172] drop2 needs backward computation.
blobs under loss: fc2
I0127 23:59:15.281761 29686 net.cpp:172] fcrelu2 needs backward computation.
blobs under loss: fc2
I0127 23:59:15.281765 29686 net.cpp:172] fc2 needs backward computation.
blobs under loss: fc1
I0127 23:59:15.281769 29686 net.cpp:172] drop1 needs backward computation.
blobs under loss: fc1
I0127 23:59:15.281772 29686 net.cpp:172] fcrelu1 needs backward computation.
blobs under loss: fc1
I0127 23:59:15.281776 29686 net.cpp:172] fc1 needs backward computation.
blobs under loss: pool4x
I0127 23:59:15.281780 29686 net.cpp:172] predrop needs backward computation.
blobs under loss: pool4
I0127 23:59:15.281785 29686 net.cpp:172] pool4 needs backward computation.
blobs under loss: conv4
I0127 23:59:15.281788 29686 net.cpp:172] relu4 needs backward computation.
blobs under loss: conv4
I0127 23:59:15.281792 29686 net.cpp:172] conv4 needs backward computation.
blobs under loss: pool3
I0127 23:59:15.281797 29686 net.cpp:172] pool3 needs backward computation.
blobs under loss: conv3
I0127 23:59:15.281801 29686 net.cpp:172] relu3 needs backward computation.
blobs under loss: conv3
I0127 23:59:15.281805 29686 net.cpp:172] conv3 needs backward computation.
blobs under loss: pool2
I0127 23:59:15.281810 29686 net.cpp:172] pool2 needs backward computation.
blobs under loss: conv2b
I0127 23:59:15.281813 29686 net.cpp:172] relu2b needs backward computation.
blobs under loss: conv2b
I0127 23:59:15.281817 29686 net.cpp:172] conv2b needs backward computation.
blobs under loss: conv2
I0127 23:59:15.281821 29686 net.cpp:172] relu2 needs backward computation.
blobs under loss: conv2
I0127 23:59:15.281826 29686 net.cpp:172] conv2 needs backward computation.
blobs under loss: pool1
I0127 23:59:15.281829 29686 net.cpp:172] pool1 needs backward computation.
blobs under loss: conv1b
I0127 23:59:15.281833 29686 net.cpp:172] relu1b needs backward computation.
blobs under loss: conv1b
I0127 23:59:15.281837 29686 net.cpp:172] conv1b needs backward computation.
blobs under loss: norm1
I0127 23:59:15.281841 29686 net.cpp:172] norm1 needs backward computation.
blobs under loss: conv1
I0127 23:59:15.281846 29686 net.cpp:172] relu1 needs backward computation.
blobs under loss: conv1
I0127 23:59:15.281849 29686 net.cpp:172] conv1 needs backward computation.
blobs under loss: data
I0127 23:59:15.281853 29686 net.cpp:174] data does not need backward computation.
I0127 23:59:15.281857 29686 net.cpp:211] This network produces output loss
I0127 23:59:15.281859 29686 net.cpp:211] This network produces output pseudoLoss
I0127 23:59:15.281873 29686 net.cpp:470] Collecting Learning Rate and Weight Decay.
I0127 23:59:15.281878 29686 net.cpp:222] Network initialization done.
I0127 23:59:15.281882 29686 net.cpp:223] Memory required for data: 1700567048
I0127 23:59:15.282316 29686 solver.cpp:151] Creating test net (#0) specified by net file: examples/plankton/inet_train_test.prototxt
I0127 23:59:15.282367 29686 net.cpp:278] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0127 23:59:15.282490 29686 net.cpp:39] Initializing net from parameters: 
name: "CaffeNet"
layers {
  top: "data"
  top: "label"
  name: "data"
  type: DATA
  data_param {
    source: "examples/plankton/plankton_val_lmdb"
    batch_size: 10
    backend: LMDB
  }
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 56
    rotate: false
    resize: 64
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv1b"
  name: "conv1b"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.02
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layers {
  bottom: "conv1b"
  top: "conv1b"
  name: "relu1b"
  type: RELU
}
layers {
  bottom: "conv1b"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.02
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "conv2b"
  name: "conv2b"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.02
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layers {
  bottom: "conv2b"
  top: "conv2b"
  name: "relu2b"
  type: RELU
}
layers {
  bottom: "conv2b"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.02
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layers {
  bottom: "conv3"
  top: "conv3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "conv4"
  name: "conv4"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.02
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layers {
  bottom: "conv4"
  top: "conv4"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "conv4"
  top: "pool4"
  name: "pool4"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool4"
  top: "pool4x"
  name: "predrop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool4x"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 1024
    weight_filler {
      type: "gaussian"
      std: 0.02
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layers {
  bottom: "fc1"
  top: "fc1"
  name: "fcrelu1"
  type: RELU
}
layers {
  bottom: "fc1"
  top: "fc1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc1"
  top: "fc2"
  name: "fc2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.02
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layers {
  bottom: "fc2"
  top: "fc2"
  name: "fcrelu2"
  type: RELU
}
layers {
  bottom: "fc2"
  top: "fc2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc2"
  top: "fc3"
  name: "fc3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 121
    weight_filler {
      type: "gaussian"
      std: 0.02
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layers {
  bottom: "fc3"
  top: "pseudoLabel"
  name: "argmax"
  type: ARGMAX
}
layers {
  bottom: "fc3"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
layers {
  bottom: "fc3"
  bottom: "pseudoLabel"
  top: "pseudoLoss"
  name: "psuedoLoss"
  type: SOFTMAX_LOSS
}
state {
  phase: TEST
}
I0127 23:59:15.282564 29686 net.cpp:67] Creating Layer data
I0127 23:59:15.282570 29686 net.cpp:359] data -> data
I0127 23:59:15.282577 29686 net.cpp:359] data -> label
I0127 23:59:15.282582 29686 net.cpp:96] Setting up data
I0127 23:59:15.282608 29686 data_layer.cpp:68] Opening lmdb examples/plankton/plankton_val_lmdb
I0127 23:59:15.282645 29686 data_layer.cpp:128] output data size: 10,1,56,56
I0127 23:59:15.282740 29686 net.cpp:103] Top shape: 10 1 56 56 (31360)
I0127 23:59:15.282745 29686 net.cpp:103] Top shape: 10 1 1 1 (10)
I0127 23:59:15.282752 29686 net.cpp:67] Creating Layer conv1
I0127 23:59:15.282754 29686 net.cpp:397] conv1 <- data
I0127 23:59:15.282759 29686 net.cpp:359] conv1 -> conv1
I0127 23:59:15.282764 29686 net.cpp:96] Setting up conv1
I0127 23:59:15.283010 29686 net.cpp:103] Top shape: 10 64 56 56 (2007040)
I0127 23:59:15.283021 29686 net.cpp:67] Creating Layer relu1
I0127 23:59:15.283025 29686 net.cpp:397] relu1 <- conv1
I0127 23:59:15.283030 29686 net.cpp:348] relu1 -> conv1 (in-place)
I0127 23:59:15.283035 29686 net.cpp:96] Setting up relu1
I0127 23:59:15.283040 29686 net.cpp:103] Top shape: 10 64 56 56 (2007040)
I0127 23:59:15.283043 29686 net.cpp:67] Creating Layer norm1
I0127 23:59:15.283046 29686 net.cpp:397] norm1 <- conv1
I0127 23:59:15.283051 29686 net.cpp:359] norm1 -> norm1
I0127 23:59:15.283056 29686 net.cpp:96] Setting up norm1
I0127 23:59:15.283059 29686 net.cpp:103] Top shape: 10 64 56 56 (2007040)
I0127 23:59:15.283063 29686 net.cpp:67] Creating Layer conv1b
I0127 23:59:15.283066 29686 net.cpp:397] conv1b <- norm1
I0127 23:59:15.283071 29686 net.cpp:359] conv1b -> conv1b
I0127 23:59:15.283077 29686 net.cpp:96] Setting up conv1b
I0127 23:59:15.283957 29686 net.cpp:103] Top shape: 10 64 56 56 (2007040)
I0127 23:59:15.283969 29686 net.cpp:67] Creating Layer relu1b
I0127 23:59:15.283973 29686 net.cpp:397] relu1b <- conv1b
I0127 23:59:15.283977 29686 net.cpp:348] relu1b -> conv1b (in-place)
I0127 23:59:15.283982 29686 net.cpp:96] Setting up relu1b
I0127 23:59:15.283985 29686 net.cpp:103] Top shape: 10 64 56 56 (2007040)
I0127 23:59:15.283990 29686 net.cpp:67] Creating Layer pool1
I0127 23:59:15.283993 29686 net.cpp:397] pool1 <- conv1b
I0127 23:59:15.283998 29686 net.cpp:359] pool1 -> pool1
I0127 23:59:15.284003 29686 net.cpp:96] Setting up pool1
I0127 23:59:15.284008 29686 net.cpp:103] Top shape: 10 64 28 28 (501760)
I0127 23:59:15.284013 29686 net.cpp:67] Creating Layer conv2
I0127 23:59:15.284015 29686 net.cpp:397] conv2 <- pool1
I0127 23:59:15.284019 29686 net.cpp:359] conv2 -> conv2
I0127 23:59:15.284024 29686 net.cpp:96] Setting up conv2
I0127 23:59:15.288633 29686 net.cpp:103] Top shape: 10 128 28 28 (1003520)
I0127 23:59:15.288645 29686 net.cpp:67] Creating Layer relu2
I0127 23:59:15.288647 29686 net.cpp:397] relu2 <- conv2
I0127 23:59:15.288652 29686 net.cpp:348] relu2 -> conv2 (in-place)
I0127 23:59:15.288656 29686 net.cpp:96] Setting up relu2
I0127 23:59:15.288661 29686 net.cpp:103] Top shape: 10 128 28 28 (1003520)
I0127 23:59:15.288666 29686 net.cpp:67] Creating Layer conv2b
I0127 23:59:15.288672 29686 net.cpp:397] conv2b <- conv2
I0127 23:59:15.288677 29686 net.cpp:359] conv2b -> conv2b
I0127 23:59:15.288681 29686 net.cpp:96] Setting up conv2b
I0127 23:59:15.292007 29686 net.cpp:103] Top shape: 10 128 28 28 (1003520)
I0127 23:59:15.292016 29686 net.cpp:67] Creating Layer relu2b
I0127 23:59:15.292021 29686 net.cpp:397] relu2b <- conv2b
I0127 23:59:15.292023 29686 net.cpp:348] relu2b -> conv2b (in-place)
I0127 23:59:15.292028 29686 net.cpp:96] Setting up relu2b
I0127 23:59:15.292032 29686 net.cpp:103] Top shape: 10 128 28 28 (1003520)
I0127 23:59:15.292037 29686 net.cpp:67] Creating Layer pool2
I0127 23:59:15.292040 29686 net.cpp:397] pool2 <- conv2b
I0127 23:59:15.292044 29686 net.cpp:359] pool2 -> pool2
I0127 23:59:15.292048 29686 net.cpp:96] Setting up pool2
I0127 23:59:15.292054 29686 net.cpp:103] Top shape: 10 128 14 14 (250880)
I0127 23:59:15.292064 29686 net.cpp:67] Creating Layer conv3
I0127 23:59:15.292068 29686 net.cpp:397] conv3 <- pool2
I0127 23:59:15.292073 29686 net.cpp:359] conv3 -> conv3
I0127 23:59:15.292076 29686 net.cpp:96] Setting up conv3
I0127 23:59:15.298682 29686 net.cpp:103] Top shape: 10 256 14 14 (501760)
I0127 23:59:15.298693 29686 net.cpp:67] Creating Layer relu3
I0127 23:59:15.298696 29686 net.cpp:397] relu3 <- conv3
I0127 23:59:15.298701 29686 net.cpp:348] relu3 -> conv3 (in-place)
I0127 23:59:15.298706 29686 net.cpp:96] Setting up relu3
I0127 23:59:15.298709 29686 net.cpp:103] Top shape: 10 256 14 14 (501760)
I0127 23:59:15.298714 29686 net.cpp:67] Creating Layer pool3
I0127 23:59:15.298717 29686 net.cpp:397] pool3 <- conv3
I0127 23:59:15.298722 29686 net.cpp:359] pool3 -> pool3
I0127 23:59:15.298725 29686 net.cpp:96] Setting up pool3
I0127 23:59:15.298730 29686 net.cpp:103] Top shape: 10 256 7 7 (125440)
I0127 23:59:15.298735 29686 net.cpp:67] Creating Layer conv4
I0127 23:59:15.298738 29686 net.cpp:397] conv4 <- pool3
I0127 23:59:15.298743 29686 net.cpp:359] conv4 -> conv4
I0127 23:59:15.298746 29686 net.cpp:96] Setting up conv4
I0127 23:59:15.324965 29686 net.cpp:103] Top shape: 10 512 7 7 (250880)
I0127 23:59:15.324988 29686 net.cpp:67] Creating Layer relu4
I0127 23:59:15.324992 29686 net.cpp:397] relu4 <- conv4
I0127 23:59:15.325000 29686 net.cpp:348] relu4 -> conv4 (in-place)
I0127 23:59:15.325005 29686 net.cpp:96] Setting up relu4
I0127 23:59:15.325009 29686 net.cpp:103] Top shape: 10 512 7 7 (250880)
I0127 23:59:15.325014 29686 net.cpp:67] Creating Layer pool4
I0127 23:59:15.325017 29686 net.cpp:397] pool4 <- conv4
I0127 23:59:15.325022 29686 net.cpp:359] pool4 -> pool4
I0127 23:59:15.325027 29686 net.cpp:96] Setting up pool4
I0127 23:59:15.325032 29686 net.cpp:103] Top shape: 10 512 3 3 (46080)
I0127 23:59:15.325037 29686 net.cpp:67] Creating Layer predrop
I0127 23:59:15.325039 29686 net.cpp:397] predrop <- pool4
I0127 23:59:15.325043 29686 net.cpp:359] predrop -> pool4x
I0127 23:59:15.325047 29686 net.cpp:96] Setting up predrop
I0127 23:59:15.325052 29686 net.cpp:103] Top shape: 10 512 3 3 (46080)
I0127 23:59:15.325057 29686 net.cpp:67] Creating Layer fc1
I0127 23:59:15.325060 29686 net.cpp:397] fc1 <- pool4x
I0127 23:59:15.325064 29686 net.cpp:359] fc1 -> fc1
I0127 23:59:15.325069 29686 net.cpp:96] Setting up fc1
I0127 23:59:15.428426 29686 net.cpp:103] Top shape: 10 1024 1 1 (10240)
I0127 23:59:15.428457 29686 net.cpp:67] Creating Layer fcrelu1
I0127 23:59:15.428462 29686 net.cpp:397] fcrelu1 <- fc1
I0127 23:59:15.428467 29686 net.cpp:348] fcrelu1 -> fc1 (in-place)
I0127 23:59:15.428472 29686 net.cpp:96] Setting up fcrelu1
I0127 23:59:15.428483 29686 net.cpp:103] Top shape: 10 1024 1 1 (10240)
I0127 23:59:15.428488 29686 net.cpp:67] Creating Layer drop1
I0127 23:59:15.428491 29686 net.cpp:397] drop1 <- fc1
I0127 23:59:15.428495 29686 net.cpp:348] drop1 -> fc1 (in-place)
I0127 23:59:15.428499 29686 net.cpp:96] Setting up drop1
I0127 23:59:15.428503 29686 net.cpp:103] Top shape: 10 1024 1 1 (10240)
I0127 23:59:15.428508 29686 net.cpp:67] Creating Layer fc2
I0127 23:59:15.428510 29686 net.cpp:397] fc2 <- fc1
I0127 23:59:15.428515 29686 net.cpp:359] fc2 -> fc2
I0127 23:59:15.428527 29686 net.cpp:96] Setting up fc2
I0127 23:59:15.440222 29686 net.cpp:103] Top shape: 10 512 1 1 (5120)
I0127 23:59:15.440232 29686 net.cpp:67] Creating Layer fcrelu2
I0127 23:59:15.440237 29686 net.cpp:397] fcrelu2 <- fc2
I0127 23:59:15.440239 29686 net.cpp:348] fcrelu2 -> fc2 (in-place)
I0127 23:59:15.440243 29686 net.cpp:96] Setting up fcrelu2
I0127 23:59:15.440248 29686 net.cpp:103] Top shape: 10 512 1 1 (5120)
I0127 23:59:15.440253 29686 net.cpp:67] Creating Layer drop2
I0127 23:59:15.440256 29686 net.cpp:397] drop2 <- fc2
I0127 23:59:15.440259 29686 net.cpp:348] drop2 -> fc2 (in-place)
I0127 23:59:15.440263 29686 net.cpp:96] Setting up drop2
I0127 23:59:15.440266 29686 net.cpp:103] Top shape: 10 512 1 1 (5120)
I0127 23:59:15.440271 29686 net.cpp:67] Creating Layer fc3
I0127 23:59:15.440274 29686 net.cpp:397] fc3 <- fc2
I0127 23:59:15.440284 29686 net.cpp:359] fc3 -> fc3
I0127 23:59:15.440291 29686 net.cpp:96] Setting up fc3
I0127 23:59:15.441679 29686 net.cpp:103] Top shape: 10 121 1 1 (1210)
I0127 23:59:15.441687 29686 net.cpp:67] Creating Layer fc3_fc3_0_split
I0127 23:59:15.441690 29686 net.cpp:397] fc3_fc3_0_split <- fc3
I0127 23:59:15.441694 29686 net.cpp:359] fc3_fc3_0_split -> fc3_fc3_0_split_0
I0127 23:59:15.441699 29686 net.cpp:359] fc3_fc3_0_split -> fc3_fc3_0_split_1
I0127 23:59:15.441704 29686 net.cpp:359] fc3_fc3_0_split -> fc3_fc3_0_split_2
I0127 23:59:15.441709 29686 net.cpp:96] Setting up fc3_fc3_0_split
I0127 23:59:15.441712 29686 net.cpp:103] Top shape: 10 121 1 1 (1210)
I0127 23:59:15.441715 29686 net.cpp:103] Top shape: 10 121 1 1 (1210)
I0127 23:59:15.441718 29686 net.cpp:103] Top shape: 10 121 1 1 (1210)
I0127 23:59:15.441722 29686 net.cpp:67] Creating Layer argmax
I0127 23:59:15.441726 29686 net.cpp:397] argmax <- fc3_fc3_0_split_0
I0127 23:59:15.441730 29686 net.cpp:359] argmax -> pseudoLabel
I0127 23:59:15.441735 29686 net.cpp:96] Setting up argmax
I0127 23:59:15.441737 29686 net.cpp:103] Top shape: 10 1 1 1 (10)
I0127 23:59:15.441743 29686 net.cpp:67] Creating Layer loss
I0127 23:59:15.441746 29686 net.cpp:397] loss <- fc3_fc3_0_split_1
I0127 23:59:15.441750 29686 net.cpp:397] loss <- label
I0127 23:59:15.441753 29686 net.cpp:359] loss -> loss
I0127 23:59:15.441757 29686 net.cpp:96] Setting up loss
I0127 23:59:15.441764 29686 net.cpp:103] Top shape: 1 1 1 1 (1)
I0127 23:59:15.441767 29686 net.cpp:109]     with loss weight 1
I0127 23:59:15.441778 29686 net.cpp:67] Creating Layer psuedoLoss
I0127 23:59:15.441781 29686 net.cpp:397] psuedoLoss <- fc3_fc3_0_split_2
I0127 23:59:15.441786 29686 net.cpp:397] psuedoLoss <- pseudoLabel
I0127 23:59:15.441788 29686 net.cpp:359] psuedoLoss -> pseudoLoss
I0127 23:59:15.441792 29686 net.cpp:96] Setting up psuedoLoss
I0127 23:59:15.441797 29686 net.cpp:103] Top shape: 1 1 1 1 (1)
I0127 23:59:15.441799 29686 net.cpp:109]     with loss weight 1
I0127 23:59:15.441803 29686 net.cpp:172] psuedoLoss needs backward computation.
blobs under loss: fc3_fc3_0_split_2
blobs under loss: pseudoLabel
I0127 23:59:15.441810 29686 net.cpp:172] loss needs backward computation.
blobs under loss: fc3_fc3_0_split_1
blobs under loss: label
I0127 23:59:15.441815 29686 net.cpp:172] argmax needs backward computation.
blobs under loss: fc3_fc3_0_split_0
I0127 23:59:15.441819 29686 net.cpp:172] fc3_fc3_0_split needs backward computation.
blobs under loss: fc3
I0127 23:59:15.441823 29686 net.cpp:172] fc3 needs backward computation.
blobs under loss: fc2
I0127 23:59:15.441828 29686 net.cpp:172] drop2 needs backward computation.
blobs under loss: fc2
I0127 23:59:15.441830 29686 net.cpp:172] fcrelu2 needs backward computation.
blobs under loss: fc2
I0127 23:59:15.441834 29686 net.cpp:172] fc2 needs backward computation.
blobs under loss: fc1
I0127 23:59:15.441838 29686 net.cpp:172] drop1 needs backward computation.
blobs under loss: fc1
I0127 23:59:15.441841 29686 net.cpp:172] fcrelu1 needs backward computation.
blobs under loss: fc1
I0127 23:59:15.441845 29686 net.cpp:172] fc1 needs backward computation.
blobs under loss: pool4x
I0127 23:59:15.441849 29686 net.cpp:172] predrop needs backward computation.
blobs under loss: pool4
I0127 23:59:15.441856 29686 net.cpp:172] pool4 needs backward computation.
blobs under loss: conv4
I0127 23:59:15.441861 29686 net.cpp:172] relu4 needs backward computation.
blobs under loss: conv4
I0127 23:59:15.441865 29686 net.cpp:172] conv4 needs backward computation.
blobs under loss: pool3
I0127 23:59:15.441869 29686 net.cpp:172] pool3 needs backward computation.
blobs under loss: conv3
I0127 23:59:15.441874 29686 net.cpp:172] relu3 needs backward computation.
blobs under loss: conv3
I0127 23:59:15.441879 29686 net.cpp:172] conv3 needs backward computation.
blobs under loss: pool2
I0127 23:59:15.441882 29686 net.cpp:172] pool2 needs backward computation.
blobs under loss: conv2b
I0127 23:59:15.441886 29686 net.cpp:172] relu2b needs backward computation.
blobs under loss: conv2b
I0127 23:59:15.441890 29686 net.cpp:172] conv2b needs backward computation.
blobs under loss: conv2
I0127 23:59:15.441895 29686 net.cpp:172] relu2 needs backward computation.
blobs under loss: conv2
I0127 23:59:15.441898 29686 net.cpp:172] conv2 needs backward computation.
blobs under loss: pool1
I0127 23:59:15.441903 29686 net.cpp:172] pool1 needs backward computation.
blobs under loss: conv1b
I0127 23:59:15.441907 29686 net.cpp:172] relu1b needs backward computation.
blobs under loss: conv1b
I0127 23:59:15.441911 29686 net.cpp:172] conv1b needs backward computation.
blobs under loss: norm1
I0127 23:59:15.441915 29686 net.cpp:172] norm1 needs backward computation.
blobs under loss: conv1
I0127 23:59:15.441922 29686 net.cpp:172] relu1 needs backward computation.
blobs under loss: conv1
I0127 23:59:15.441926 29686 net.cpp:172] conv1 needs backward computation.
blobs under loss: data
I0127 23:59:15.441931 29686 net.cpp:174] data does not need backward computation.
I0127 23:59:15.441933 29686 net.cpp:211] This network produces output loss
I0127 23:59:15.441936 29686 net.cpp:211] This network produces output pseudoLoss
I0127 23:59:15.441951 29686 net.cpp:470] Collecting Learning Rate and Weight Decay.
I0127 23:59:15.441956 29686 net.cpp:222] Network initialization done.
I0127 23:59:15.441959 29686 net.cpp:223] Memory required for data: 66428408
I0127 23:59:15.442010 29686 solver.cpp:41] Solver scaffolding done.
I0127 23:59:15.442018 29686 solver.cpp:160] Solving CaffeNet
I0127 23:59:15.442020 29686 solver.cpp:161] Learning Rate Policy: step
I0127 23:59:15.442039 29686 solver.cpp:264] Iteration 0, Testing net (#0)
I0127 23:59:21.289330 29686 solver.cpp:315]     Test net output #0: loss = 4.81086 (* 1 = 4.81086 loss)
I0127 23:59:21.289357 29686 solver.cpp:315]     Test net output #1: pseudoLoss = 4.50863 (* 1 = 4.50863 loss)
F0127 23:59:21.492841 29686 softmax_loss_layer.cpp:62] SOFTMAX_LOSS Layer cannot backpropagate to label inputs.
*** Check failure stack trace: ***
    @     0x7f86d2650daa  (unknown)
    @     0x7f86d2650ce4  (unknown)
    @     0x7f86d26506e6  (unknown)
    @     0x7f86d2653687  (unknown)
    @           0x4cf20e  caffe::SoftmaxWithLossLayer<>::Backward_cpu()
    @           0x479b9c  caffe::Net<>::BackwardFromTo()
    @           0x46fe26  caffe::Solver<>::Solve()
    @           0x417472  train()
    @           0x4110e1  main
    @     0x7f86ce250ec5  (unknown)
    @           0x415297  (unknown)
    @              (nil)  (unknown)
Aborted (core dumped)
